import numpy
import scipy.stats
import scipy.linalg
import itertools
import copy
import pyopencl as cl
import time

# (MUCH) Faster permutation methods, relying on OpenCL to offload computation to GPU.
# ONLY WORKS for correlation right now. You can drop in other metrics by writing
# new OpenCL kernels (see cor.cl)
def loadProgram(ctx,filename):
	"""Load an OpenCL program stored as a text file into the specified context.

	:param ctx: OpenCL context
	:param filename: file name
	
	:returns: OpenCL program
	"""
	
	f = open(filename, 'r')
	fstr = "".join(f.readlines())
	#print fstr
	return cl.Program(ctx, fstr).build()




def perm_cor(datain,deviceno=1,num_perms=1,kernelfile='cor.cl'):
	"""
	Use OpenCL to build a correlation matrix with permutation testing.
	So... after running this, if, e.g.,  you wanted to know if the correlation between
	timeseries 5 and 6 was significant, you'd check:
		result[0,5,6]
	against
		 hist(result[:,5,6])
		 
	:param data: Array of timeseries. [num_ts,nt]
	:param deviceno: which OpenCL device should we use?
	:param num_perms: number of permutations
	:param kernelfile: name of file storing the OpenCL correlation kernel
	
	:return: result[num_perms, num_ts, num_ts]. Note: result[0] is the 'real' correlation matrix (null permutation)
	
	"""
	import copy
	
	# Current GPUs are single-precision only
	data = numpy.float32(datain)
	
	# Extract dimensions of input data and flatten array into 1D row-major
	num_ts = data.shape[0]
	nt = data.shape[1]
	vts = data.flatten()
	
	# Grab devices from first OpenCL platform
	devices = cl.get_platforms()[0].get_devices()
	print 'Firing up', devices[deviceno].name
	
	# Create OpenCL context and command queue
	ctx = cl.Context([devices[deviceno]])
	queue = cl.CommandQueue(ctx)

	# Load kernel
	program=loadProgram(ctx,kernelfile)

	# Create some fake voxel timeseries
	perm = numpy.zeros((num_perms,num_ts,nt),dtype='float32')
		
	result = numpy.zeros(num_ts*num_ts*num_perms,dtype='float32')
	print result.shape


	# create and load OpenCL buffers
	mf = cl.mem_flags
	vts_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=vts)
	nt_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=numpy.int32(nt))
	perm_buf = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=perm)

	result_buf = cl.Buffer(ctx, mf.WRITE_ONLY, result.nbytes)


	# Local dimensions (hack to accomodate CPU error)
	if deviceno != 0:
		locald = (1,1,1)
	else:
		locald = (10,10,1)

		
	# Generate set of permutations
	print 'Generating permutations...'
	start = time.time()
	perm[0,:,:] = copy.deepcopy(data)
	for j in range(1,num_perms):
			ptemp = copy.deepcopy(data)
			for r in range(num_ts):
				ptemp[r] = numpy.random.permutation(ptemp[r])
			perm[j,:,:] = copy.deepcopy(ptemp)	

	perm = perm.flatten()
	print '... took' , (time.time() - start) , 's\n'
	
	print 'Loading permutations to compute unit...'
	start = time.time()
	# Upload all permutations
	cl.enqueue_copy(queue, perm_buf, perm).wait()
	print '... took' , (time.time() - start) , 's\n'

	print 'Computing and loading result'
	start = time.time()
	# Compute correlations
	program.correlate(queue, (num_ts,num_ts,num_perms), locald, vts_buf, perm_buf, result_buf, nt_buf, cl.LocalMemory(4*nt*locald[0]), cl.LocalMemory(4*nt*locald[0]))
		

			
	# Blocking read of result
	cl.enqueue_copy(queue, result, result_buf).wait() 
	print '... took' , (time.time() - start) , 's\n'

	return result.reshape(num_perms,num_ts,num_ts)



def perm_thresh(perms,alpha=0.05):
	"""
	Given a 3D permutation dataset generated by perm_cor, return a thresholded
	correlation matrix by testing _individual timeseries/voxels_. 
	Note that this DOES NOT correct for multiple comparisons! Each timeseries is
	tested against it's own, personal, null distribution.
	"""
	
	# strip out the real correlation matrix from the nulls
	result = perms[1:,:,:]
	
	threshmat = numpy.zeros((perms.shape[1],perms.shape[1]))
	
	# For the upper triangle of the matrix
	for i in range(perms.shape[1]):
		for j in range(i,perms.shape[1]):
			# Compute the significant threshold from the null distributions and test the actual value
			thr = numpy.sort(numpy.abs(result[:,i,j]))[result[:,i,j].shape[0]*(1-alpha)]
			if numpy.abs(perms[0,i,j]) > thr:
				if perms[0,i,j] > 0:
					threshmat[i,j] = 1
				else:
					threshmat[i,j] = -1
	
	return threshmat


def get_thresh_perm_max_voxel(perms,alpha=0.05):
	"""
	Given a 3D permutation dataset generated by perm_cor, return a _single_ threshold, 
	for all timeseries, derived from the permutation distribution of the maximal voxel statistic. 
	This threshold DOES control for multiple comparisons.
	"""
	# strip out the real correlation matrix from the nulls
	result = numpy.abs(perms[1:,:,:])
	
	m = []
	for i in range(perms.shape[1]):
	 for j in range(i,perms.shape[1]):
		m.append(numpy.max(result[:,i,j]))
		
	m=numpy.sort(numpy.array(m))
	
	return m[alpha*m.shape[0] + 1]



# REEEEALY SLOW Python permutation testing -- but very general.
def permutation(x,y,func=scipy.stats.pearsonr,perms=1000,alpha=0.05):
	yp = copy.deepcopy(y)
	
	# Permute y, compare to X and store
	rs = numpy.zeros(perms)
	for i in xrange(perms):
		numpy.random.shuffle(yp)
		rs[i] = func(x,yp)[0]
	
	# Is the real correlation significant?
	p,t = scipy.stats.ttest_1samp(rs,func(x,y))
	
	# FIXME DEBUG CHECK THIS
	return ( (numpy.fabs(p[1]) < alpha), p)

# Removed p-value calculation from scipy.stats.r; slight speed improvement
# FIXME UGLY HACK FOR A BIT OF SPEED
def fast_r(x,y):
    # x and y should have same length.
    x = numpy.asarray(x)
    y = numpy.asarray(y)
    n = len(x)
    mx = x.mean()
    my = y.mean()
    xm, ym = x-mx, y-my
    r_num = n*(numpy.add.reduce(xm*ym))
    r_den = n*numpy.sqrt(scipy.stats.ss(xm)*scipy.stats.ss(ym))
    r = (r_num / r_den)

    # Presumably, if abs(r) > 1, then it is only some small artifact of floating
    # point arithmetic.
    r = max(min(r, 1.0), -1.0)
    return [r]
    
    

# Bootstrapping methods
# FIXME; there are better ways to do this


def bootstrap(x,y,func=scipy.stats.pearsonr,boots=1000,conf_int=0.95):
	"""
	Bootstrap 'func' between x and y.
	
	:param x: timeseries
	:param y: timeseries
	:param func: comparison function for the timeseries
	:param boots: number of bootstraps
	:param conf_int: desired confidence interval
	
	:returns: (Boolean: _actual value signficiant?_, bottom CI, top CI, actual value)
	"""
	
	# Resample with replacement
	index = numpy.random.randint(x.shape[0],size=(boots,x.shape[0]))
	bx = x[index]
	by = y[index]

	res=[]
	
	# Bootstrap! (This could be done much faster without the list comprehension...) 
	res =  [ func(bx[i],by[i])[0] for i in xrange(boots) ]
	
	# FIXME DEBUG check this is correct
	res = numpy.sort(numpy.array(res))
	lowci = res[boots-int(conf_int*boots)]
	highci = res[int(conf_int*boots)]
	return (not (lowci <= 0 <= highci), lowci, highci, func(x,y)[0])
	
def block_bootstrap(x,y,func=scipy.stats.pearsonr,boots=1000,conf_int=0.95,blocksize=10):
	"""
	Block bootstrap 'func' between x and y.
	(viz. instead of resampling individual timepoints from the data set, resample
	_contiguous blocks_)
	
	FIXME: if we want to get really hardcore, we could do stationary bootstraps too...
	
	:param x: timeseries
	:param y: timeseries
	:param func: comparison function for the timeseries
	:param boots: number of bootstraps
	:param conf_int: desired confidence interval
	:param blocksize: the size of block to permute
	
	:returns: (Boolean: _actual value signficiant?_, bottom CI, top CI, actual value)
	"""
	
	# Block size must divide ts size
	assert x.shape[0] % blocksize == 0
	
	res = []
	
	# For each boot
	for b in xrange(boots):
	
		# Resample BLOCKS, with replacement
		index = numpy.random.randint( x.shape[0]/blocksize, size=x.shape[0]/blocksize)
		bx = numpy.array([])
		by = numpy.array([])
		
		# Create block-resampled timeseries and compare
		for i in index:
			bx = numpy.append(bx,x[i*blocksize:(i+1)*blocksize])
			by = numpy.append(by,y[i*blocksize:(i+1)*blocksize])
		res.append( func(bx,by)[0])
		
	
	# FIXME DEBUG check this is correct
	res = numpy.sort(numpy.array(res))
	lowci = res[boots-int(conf_int*boots)]
	highci = res[int(conf_int*boots)]
	return (not (lowci <= 0 <= highci), lowci, highci, func(x,y)[0])
	
	
	
def boot_tri(ts,func=fast_r,boots=1000,conf_int=0.95,bootfunc=bootstrap):	
	"""
	Build a trinarized matrix using bootstrapping as a threshold technique.
	
	:param ts: [channel, time] - timeseries matrix 
	:param func: comparison function for the timeseries
	:param boots: number of bootstraps
	:param conf_int: desired confidence interval
	:param bootfunc: pairwise bootstrapping function (e.g., 'bootstrap' or 'block_bootstrap')
	
	:returns: trinarized matrix
	"""	
	
	trimat = numpy.zeros((ts.shape[0],ts.shape[0]))
	
	# Bootstrap each entry in the matrix
	# FIXME -- slow; but the bottleneck is in your timeseries comparison
	# function. This is the price of generality. We either live with slow and
	# general, or scipy.weave in some C for correlation bootstrapping.
	for i in xrange(ts.shape[0]):
		for j in xrange(i,ts.shape[0]):
			t = bootfunc(ts[i],ts[j],func,boots,conf_int)
			if t[0]:
				if t[3] > 0:
					trimat[i][j] = 1
				else:
					trimat[i][j] = -1
	
	return trimat


def per_entry_p(ts, alpha):
	"""
	Build a trinarized matrix using per-entry p-value as a threshold technique.
	
	:param ts: [channel, time] - timeseries matrix 
	:param alpha: p-value threshold
	trimat = numpy.zeros((ts.shape[0],ts.shape[0]))
	"""
	trimat = numpy.zeros((ts.shape[0],ts.shape[0]))
	
	for i in xrange(ts.shape[0]):
		for j in xrange(i,ts.shape[0]):
			p = scipy.stats.pearsonr(ts[i],ts[j])[1]
			if numpy.fabs(p)<alpha:
				if p > 0:
					trimat[i][j] = 1
				else:
					trimat[i][j] = -1	

	return trimat

# Standard Bonferroni correction (p < .05) 
def bc_alpha(mat,alpha):
	"""
	Find the threshold for a correlation matrix, given an uncorrected alpha and using
	a Bonerroni corretion.
	
	:param mat: Correlation Matrix (MUST BE A _CORRELATION_ MATRIX!)
	:param alpha: desired uncorrected alpha

	:returns: a corrected alpha
	"""
	
	# Number of comparisons
	# FIXME count diagonals, or no?
	n = (mat.shape[0]*mat.shape[0] + mat.shape[0])/2
	
	# correct alpha
	return alpha/n
	
	
	
		
# Binarizing at > or < 90th percentile 
def percent_thresh(mat, percentile=90,symmetric=True):
	"""
	Find the threshold that preserves values in a given percentile.
	
	:param mat: metric matrix
	:param percentile: percentile
	
	:returns: threshold
	"""
	
	if symmetric:
		m= mat[numpy.triu_indices(mat.shape[0],k=1)]
	else:
		m= mat.flatten()
		
	return scipy.stats.scoreatpercentile(m,percentile)

# Binarizing at  R = > .20 
# Don't need to do anything for this. Just call 'trinarize'




# (my thoughts) Monte-Carlo Permutation?


# FDR
# 1. Sort P values in ascending order
# Find largest i s.t. P_i < alpha * i / n_comp
# n_comp is the number of comparisons (n^2 - n)/2

def RtoP(r,nTRs):
	"""Given a Pearson R value, and the number of TRs used to derive said value,
	   convert it into a p-value
	   
	   :param r: a Pearson R score
	   :param nTRs: length of the time series from which R was computed
	   
	   :returns: p-value
	   """
	# r to p
	df = nTRs-2
	if abs(r) == 1.0:
		p = 0.0
	else:
		t_squared = r*r * (df / ((1.0 - r) * (1.0 + r)))
		p = scipy.stats.betai(0.5*df, 0.5, df / (df + t_squared))
	return p        
        
        
def FDR_thresh_corrmat(mat,nTRs,alpha=0.05):
	"""Thresholds a correlation matrix 'mat', based on timseries with 'nTRs' entries
	   with a chosen 'alpha' based on False Discovery Rate.
	   
	:param mat: upper-triangular adjacency matrix
	:param nTRs: length (in datapoints) of time series used to generate 'mat'
	:returns: threshold value to achieve target FDR.
	"""
	
	edges = mat[numpy.triu_indices(mat.shape[0],k=1)]
	pvals = numpy.array([ RtoP(r,nTRs) for r in edges ])
	sort_index=numpy.argsort(pvals)
	pvals=pvals[sort_index]
	edges=edges[sort_index]
	
	for i in range(len(pvals)-1,0,-1):
		if pvals[i] < alpha * i / len(edges):
			#print 'Cutting ', i, pvals[i]
			return edges[i]
	
	return 0.0



def S_thresh(mat,S):
	"""Return threshold for an upper-triangular adjacency matrix with target S-value.
	
	:param mat: upper-triangular adjacency matrix
	:param S: target S-value
	:returns: threshold value to achieve target S
	"""
	
	edges = numpy.sort(mat[numpy.triu_indices(mat.shape[0],k=1)])
	
	target_degree = numpy.exp(numpy.log(mat.shape[0])/S)
	num_edges = int((mat.shape[0]*target_degree)/2.0)
	
	return edges[len(edges)-num_edges]



def unfold_matrix(A,n):
	"""Unfold a matrix 'A', using an n-bin CDF approximation, and return the nearest neighbor spacings.
	
	:param A: matrix
	:param n: number of bins
	:returns: The NNS of the eigenvalues
	"""
	# Get eigenvalues of matrix
	#E,v=numpy.linalg.eig(A) # FIXME HACKING!!!
	E,v=scipy.linalg.eig(A)
	E=numpy.real(E)
	
	# Chunk histogram bins
	YR = numpy.linspace(E.min(),E.max(),n) 
	
	# Create an n-binned cumulative 'distribution'
	CumDist = numpy.zeros(n)
	
	for j in range(n):
		CumDist[j] = float(((numpy.nonzero( E.flatten() <= YR[j] ))[0]).size)/A.shape[0]
	
	# Fit a cubic polynomial to the cumulative distribution
	# FIXME -- can use higher-order polynomial if desired
	p = numpy.polyfit(YR,CumDist,3)
	
	# 'Unfold' by evaluating the polynomial on the bins and the eigenvalues 
	FitDist = numpy.polyval(p,YR)
	xi = numpy.sort(numpy.polyval(p,E))
	
	# Find nearest-neighbor spacings
	nns = numpy.diff(xi).flatten()
	
	return nns
	

	
def Ppois(s):
	return exp(-s)
	
def PGOE(s):
	return 0.5 * pi * s * exp(-pi*s**2/4.0)





def RMT_thresh(rawmat, min_thresh, max_thresh, nthresh,nbins=15):
	""" **FIXME** -- still an exploratory hack function
	Input a raw (symmetric) matrix 'rawmat', minimum and maximum thresholds
	to explore and the number of steps between min_ and max_thresh.
	Can optionally specify the number of bins to use in the unfolding of the NNSD.
	For each threshold:
		- Thresholds matrix
		- Gets nearest-neighbor eigenvalue spacings
		- Tests to see if the NNS came from an exponential distribution
		- Reports significant matches
		- Plots 'exponentialness' of NNSD against increasing threshold
		
	:param rawmat:  a raw (symmetric) matrix
	:param min_thresh: minimum threshold
	:param max_thresh: maximum threshold
	:param nthresh: number of threshold steps between min and max to explore
	:param nbins: number of bins to use in NNSD unfolding (recommended default 15) 
	:returns: A list of the Anderson-Darling statistic scores and associated thresholds

	"""
	tlist = numpy.linspace(min_thresh, max_thresh, nthresh)
	
	# Parameters
	mavg = 4 # Number of values in moving average for plot
	
	maxp = 0
	maxthresh = 0
	adlist = []
	
	# For each threshold level
	for thresh in tlist:
	
		# Threshold matrix
		tmat = rawmat.copy()
		tmat[abs(tmat)<thresh] = 0

		# Unfold upper-triangular representaiton
		#tmat = tmat + tmat.transpose() - numpy.diag(tmat.diagonal())
		

		# Get unfolded nearest-neighbor spacings
		nns = unfold_matrix(tmat,nbins)
		
		
		# Were the nearest-neighbor spacings drawn from an exponential 
		# distribution?
		adscore = scipy.stats.anderson(nns,dist='expon')
		
		# If any of our Anderson-Darling test scores are below a
		# p=0.05 critical point, dump the info to stdout
		if True in [ adscore[0] < x for x in adscore[1][0:3]]:
			#print 'Thresh:',thresh, "A-D statistic:", adscore[0]
			#print adscore[1],adscore[2]
			#print ''
			pass
		else:
			pass
			
		adlist.append(adscore[0])
		
		# To plot the actual nnsd, do:		
		#Y,X = normhist(nns,nbins)
		#plot(X,Y)
		
	# Print out the best threshold
	best = numpy.array(adlist).argmin()
	print "Best threshold: ", tlist[best], "(AD score ", adlist[best],")"

	return [adlist,tlist]

def glasso(ts):	
	"""
	Return a sparse covariance matrix, estimated by graphical lasso, with cross-validated parameter
	selection.
	
	:param ts: [channel, time] - timeseries matrix 
	:param
	:returns: covariance matrix
	"""
	import sklearn.covariance
	
	GL=sklearn.covariance.GraphLassoCV()
	GL.fit(ts.transpose())
	
	return GL.covariance_
	

